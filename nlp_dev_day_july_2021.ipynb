{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_dev_day_july_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachelpopa/2016WoW/blob/master/nlp_dev_day_july_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXaYrQdNGyJD"
      },
      "source": [
        "# Natural Language Processing Dev Day\n",
        "\n",
        "Welcome to the NLP Dev day. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGRrbJrgQrDI"
      },
      "source": [
        "Natural Language Processing (NLP) is a subfield of AI which explores how to make computers \"understand\" natural languages, such as English. \n",
        "\n",
        "This tutorial is meant to walk you through some of the basic concepts used in practice to process text documents. The first section is about pre-processing data, the second is mostly about classifying data. We will be using Python NLTK (Natural Language Toolkit) and scikit-learn (or sklearn), a machine learning library. \n",
        "\n",
        "Start by making your own copy of this notebook in Google Colab so that you can edit/experiment with any and all of the code snippets (you will need a Google drive account to do this). Reach out in Teams if you have questions or notice a mistrake!\n",
        "\n",
        "After you've gone through the tutorial, you can spin up a project of your own with a dataset of your choice.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uof8DgniGxr-"
      },
      "source": [
        "### Getting Started\n",
        "\n",
        "Run the snippet below to get set up with some of the libraries & datasets we will be using. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0XCY7oTR40M",
        "outputId": "40e151eb-450f-4371-cd75-1d1454f547b3"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"names\")\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KwObR1-VAOe"
      },
      "source": [
        "## Section One: Pre-processing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk-_GgOYJPXb"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization means splitting text into pieces (or 'tokens') that we are interested in analyzing. Tokens might be sentences, or they might be individual words. \n",
        "\n",
        "Feel free to experiment with some of the tokenizers below. For the most part, `WordPunctTokenizer` will split special characters (such as apostrophes) into seperate tokens, while `word_tokenize` will try to keep them attached to the relevant words. See [here](https://stackoverflow.com/questions/50240029/nltk-wordpunct-tokenize-vs-word-tokenize) for more about why. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldhweJWyHEKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d54805-56d8-49e2-a3c4-42bf64ceedef"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, \\\n",
        "        word_tokenize, WordPunctTokenizer\n",
        "\n",
        "input_text = \"Here's some input text, we can use it to see what tokenization is.\" \n",
        "\n",
        "print(\"\\nSentence tokenizer:\")\n",
        "print(sent_tokenize(input_text))\n",
        "\n",
        "print(\"\\nWord tokenizer:\")\n",
        "print(word_tokenize(input_text))\n",
        "\n",
        "print(\"\\nWord punct tokenizer:\")\n",
        "print(WordPunctTokenizer().tokenize(input_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence tokenizer:\n",
            "[\"Here's some input text, we can use it to see what tokenization is and how it works.\"]\n",
            "\n",
            "Word tokenizer:\n",
            "['Here', \"'s\", 'some', 'input', 'text', ',', 'we', 'can', 'use', 'it', 'to', 'see', 'what', 'tokenization', 'is', 'and', 'how', 'it', 'works', '.']\n",
            "\n",
            "Word punct tokenizer:\n",
            "['Here', \"'\", 's', 'some', 'input', 'text', ',', 'we', 'can', 'use', 'it', 'to', 'see', 'what', 'tokenization', 'is', 'and', 'how', 'it', 'works', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDZ4XncNeNfh"
      },
      "source": [
        "### Removing Stop Words\n",
        "\n",
        "**Stop words** are words that are so commonly used that they are useless for most applications. Words such as *the*, *of*, and *is* tell us very little about what a document is about. We'd often like to simply remove them.\n",
        "\n",
        "There is a pre-defined list of stop words available in NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBYwND4H8l1n",
        "outputId": "a3510622-7e3f-4e8b-b0e3-586393aa770c"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "example_sent = \"Like most sentences, this sentence contains a few stop words that aren't very interesting.\"\n",
        "\n",
        "tokens = word_tokenize(example_sent)\n",
        " \n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "filtered_sentence = [w for w in tokens if not w.lower() in stop_words]\n",
        " \n",
        "print(tokens)\n",
        "print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Like', 'most', 'sentences', ',', 'this', 'sentence', 'contains', 'a', 'few', 'stop', 'words', 'that', 'are', \"n't\", 'very', 'interesting', '.']\n",
            "['Like', 'sentences', ',', 'sentence', 'contains', 'stop', 'words', \"n't\", 'interesting', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3279MjMSw7u"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "In linguistics, the **stem** of a word is the part of a word responsible for its lexical meaning. It's the part of the word that's leftover when you remove prefixes and suffixes, and the part of the word that's leftover when you de-conjugate a verb. For example, the stem of *walking* is *walk*, the stem of *quickly* is *quick*. In English, the stem of a word is often also a word, but not always.\n",
        "\n",
        "Stemming is a common preprocessing step when working with text data. It's useful to ignore prefixes, suffixes, and verb tense in a lot of applications; if someone is searching for documents about \"organizing\", we might as well return documents that are about \"organize\", \"organized\", \"organizer\", etc. \n",
        "\n",
        "`PorterStemmer`, `LancasterStemmer`, and `SnowballStemmer` are three stemmers available in NLTK. Feel free to compare them below. Check out this [stackoverflow article](https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg) for more on the differences between the Porter, Lancaster, and Snowball algorithms. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drRqvBLIS929",
        "outputId": "0d62d7b0-a71f-48f0-a1a0-28e887e56115"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "input_words = ['chocolate', 'hat', 'walking', 'landed', 'growth', 'messenger', \n",
        "        'possibly', 'provision', 'building', 'kept', 'scratchy', 'code', 'lying']\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "\n",
        "\n",
        "stemmer_names = ['Porter', 'Lancaster', 'Snowball']\n",
        "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
        "print('\\n', formatted_text.format('Input', *stemmer_names), \n",
        "        '\\n', '='*68)\n",
        "\n",
        "for word in input_words:\n",
        "    output = [word, porter.stem(word), \n",
        "            lancaster.stem(word), snowball.stem(word)]\n",
        "    print(formatted_text.format(*output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "            Input          Porter       Lancaster        Snowball \n",
            " ====================================================================\n",
            "       chocolate          chocol          chocol          chocol\n",
            "             hat             hat             hat             hat\n",
            "         walking            walk            walk            walk\n",
            "          landed            land            land            land\n",
            "          growth          growth            grow          growth\n",
            "       messenger         messeng         messeng         messeng\n",
            "        possibly         possibl            poss         possibl\n",
            "       provision          provis          provid          provis\n",
            "        building           build           build           build\n",
            "            kept            kept            kept            kept\n",
            "        scratchy        scratchi        scratchy        scratchi\n",
            "            code            code             cod            code\n",
            "           lying             lie           lying             lie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j96AoTET8Qs"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "We can take stemming one step further by making sure the result is actually a real word. This is known as **lemmatization**. Lemmatization is slower than stemming, but sometimes it's useful.\n",
        "\n",
        "The `WordNetLemmatizer` removes prefixes and suffixes only if the resulting word is in its dictionary. It also tries to remove tenses from verbs and convert plural nouns to singular. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOxWWTHaUDRL",
        "outputId": "4e1ae0b4-183a-44f9-aacb-239bef5b1c4f"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "input_words = ['chocolate', 'hats', 'walking', 'landed', 'women', 'messengers', \n",
        "        'possibly', 'provision', 'building', 'kept', 'scratchy', 'code', 'lying', 'Frisco']\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatizer_names = ['Noun Lemmatizer', 'Verb Lemmatizer']\n",
        "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
        "print('\\n', formatted_text.format('Input', *lemmatizer_names), \n",
        "        '\\n', '='*75)\n",
        "\n",
        "for word in input_words:\n",
        "  output = [word, lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='v')]\n",
        "  print(formatted_text.format(*output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                    Input         Noun Lemmatizer         Verb Lemmatizer \n",
            " ===========================================================================\n",
            "               chocolate               chocolate               chocolate\n",
            "                    hats                     hat                     hat\n",
            "                 walking                 walking                    walk\n",
            "                  landed                  landed                    land\n",
            "                   women                   woman                   women\n",
            "              messengers               messenger              messengers\n",
            "                possibly                possibly                possibly\n",
            "               provision               provision               provision\n",
            "                building                building                   build\n",
            "                    kept                    kept                    keep\n",
            "                scratchy                scratchy                scratchy\n",
            "                    code                    code                    code\n",
            "                   lying                   lying                     lie\n",
            "                  Frisco                  Frisco                  Frisco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYhY6bOopYR0"
      },
      "source": [
        "### Part-of-Speech Tagging\n",
        "\n",
        "A part-of-speech tagger (AKA a **POS tagger**) attaches a part-of-speech tags to words, meaning it labels nouns as nouns, verbs as verbs, etc. Try out NLTK's POS tagger below. Under the hood, a tagger is a machine learning model. When you give it a word, it predicts what type of word it is. \n",
        "\n",
        "POS tags are useful in a number of ways. For instance, suppose NLTK runs into a word it's never seen before: *He was scrobbling*. Even though it has no idea of the meaning, it's likely to guess that *scrobbling* is a verb. Additionally, POS tags help us distinguish between homonymns. Consider this sentence: *They refuse to permit us to obtain the refuse permit*. The first *refuse* is a verb, the second *refuse* is a noun. Depending on how picky we are, we might want to consider them as completely different words in our system. \n",
        "\n",
        "\n",
        "The example below uses NLTK's Averaged Perceptron Tagger (a *perceptron* is a neural network consisting of only one layer). If you're interested in how it works, [this article](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python) explains how to write an averaged perceptron tagger.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpM9WOw3qDNU",
        "outputId": "d26a6be1-7dab-4b58-b53b-2c09d9d9174f"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Uncomment this to see descriptions of all the parts of speech in the tagger\n",
        "# notice how some of the verbs include extra information, like verb tense (present, progressive, past, etc)\n",
        "# nltk.help.upenn_tagset()\n",
        "\n",
        "tokens = word_tokenize(\"Let's look at part-of-speech tagging.\")\n",
        "\n",
        "print(nltk.pos_tag(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Let', 'VB'), (\"'s\", 'POS'), ('look', 'VB'), ('at', 'IN'), ('part-of-speech', 'JJ'), ('tagging', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmmrPr9SVKFH"
      },
      "source": [
        "### Count Vectorizer\n",
        "\n",
        " `CountVectorizer` (from the [sklearn](https://scikit-learn.org/stable/) library) converts a documents into \"vectors\" of term/token counts.\n",
        "\n",
        " CountVectorizer is useful for creating a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix). A document-term matrix is handy when you want to represent your data numerically, and it is often passed to machine learning algorithms (read: we will be using CountVectorizer in later examples). \n",
        "\n",
        " CountVectorizer does a few handy things by default, including: \n",
        "\n",
        "*   converts your text to lowercase\n",
        "*   does word tokenization for you\n",
        "*   gets rid of single characters (meaning words like 'a' and 'I' are discarded)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZlkUEaGXP8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb76fad-c2b3-4e02-a1fd-1604fa4f4fc6"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Each sentence here is considered a 'document'\n",
        "cat_in_the_hat_docs=[\n",
        "       \"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
        "       \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
        "       \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
        "       \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
        "       \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" \n",
        "      ]\n",
        "\n",
        "cv = CountVectorizer(cat_in_the_hat_docs)\n",
        "# .fit creates a vocabulary, that is, picks out all the unique words in each document and assigns them an index\n",
        "vectorizer = cv.fit(cat_in_the_hat_docs)\n",
        "# .fit_transform creates a document-term matrix, meaning it picks out all the unique words and returns a 2D array where \n",
        "# each row represents a document & each column represents a term/word in the vocabulary\n",
        "count_vector=cv.fit_transform(cat_in_the_hat_docs)\n",
        "\n",
        "# Print unique words with their indices\n",
        "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
        "\n",
        "# Print the document-term matrix\n",
        "print(count_vector.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary:  {'one': 28, 'cent': 8, 'two': 40, 'cents': 9, 'old': 26, 'new': 23, 'all': 1, 'about': 0, 'money': 22, 'cat': 7, 'in': 16, 'the': 37, 'hat': 13, 'learning': 19, 'library': 20, 'inside': 18, 'your': 42, 'outside': 30, 'human': 15, 'body': 4, 'oh': 25, 'things': 39, 'you': 41, 'can': 6, 'do': 10, 'that': 36, 'are': 2, 'good': 12, 'for': 11, 'staying': 34, 'healthy': 14, 'on': 27, 'beyond': 3, 'bugs': 5, 'insects': 17, 'there': 38, 'no': 24, 'place': 31, 'like': 21, 'space': 33, 'our': 29, 'solar': 32, 'system': 35}\n",
            "[[1 1 0 0 0 0 0 1 3 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            "  0 1 0 0 1 0 0]\n",
            " [1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 2 0 0 0 0 1]\n",
            " [1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
            "  1 2 0 1 0 2 0]\n",
            " [1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1\n",
            "  0 1 1 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGAI9aVj4S6p"
      },
      "source": [
        "### Keyword Extraction (TF-IDF)\n",
        "\n",
        "Keyword extraction is a common pre-processing step and a common standalone task in NLP. It means picking out important words from a document that describe what the document is about. \n",
        "\n",
        "**Term Frequency-Inverse Document Frequency (TF-IDF)** is essentially a statistic assigned to a word that indicates how important it is to a document. Words with a high TF-IDF score are considered to be keywords.  \n",
        "\n",
        "[The first 5 minutes of this video](https://www.youtube.com/watch?v=RPMYV-eb6lI) give a pretty good explanation of how TF-IDF is computed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "UWIirtusCJQd",
        "outputId": "763ea136-b778-40f2-b4e0-e6016b532acf"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "# We should note that TF-IDF works a lot better on larger datasets\n",
        "docs=[\"the house had a tiny little mouse\", \n",
        "\"the cat saw the mouse\", \n",
        "\"the mouse ran away from the house\", \n",
        "\"the cat finally ate the mouse\", \n",
        "\"the end of the mouse story\"\n",
        "]\n",
        " \n",
        "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
        " \n",
        "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n",
        "\n",
        "# Get the vector for the first document\n",
        "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
        "\n",
        "# Using a pandas dataframe to pretty print\n",
        "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
        "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>had</th>\n",
              "      <td>0.493562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>little</th>\n",
              "      <td>0.493562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tiny</th>\n",
              "      <td>0.493562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>house</th>\n",
              "      <td>0.398203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mouse</th>\n",
              "      <td>0.235185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.235185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ate</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>away</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>end</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>finally</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>from</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ran</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>saw</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>story</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            tfidf\n",
              "had      0.493562\n",
              "little   0.493562\n",
              "tiny     0.493562\n",
              "house    0.398203\n",
              "mouse    0.235185\n",
              "the      0.235185\n",
              "ate      0.000000\n",
              "away     0.000000\n",
              "cat      0.000000\n",
              "end      0.000000\n",
              "finally  0.000000\n",
              "from     0.000000\n",
              "of       0.000000\n",
              "ran      0.000000\n",
              "saw      0.000000\n",
              "story    0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_U6O1RUvWE_"
      },
      "source": [
        "### Task: Pick a dataset you want to work with\n",
        "\n",
        "Before reading into the next section, it might be helpful to pick a dataset you want to experiment with. Go ahead and search online for a dataset you are interested in using. You'll want to find one that contains raw text data. Keep your dataset in mind when going through the examples in the next section. Which (if any) of the tasks below are applicable to it? \n",
        "\n",
        "NLTK contains a set of [built-in datasets](http://www.nltk.org/nltk_data/) for experimentation and learning which might be a good starting point (they are mostly geared towards very specific tasks). There's also [Kaggle](https://www.kaggle.com/datasets) and [Google Dataset Search](https://datasetsearch.research.google.com/). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idasOO0OE1Z0"
      },
      "source": [
        "## Part two: Classification and Modeling\n",
        "\n",
        "In this section we will walk through a few examples of classification and one example of topic modelling. \n",
        "\n",
        "All of the classification examples below use a Naive Bayes model for classification. For the purposes of this dev day, the model itself isn't important. A lot of what we are learning today is about how to get text data into a useful format for passing to a classifier like Naive Bayes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZv5CSr2Wa8G"
      },
      "source": [
        "### Category Prediction\n",
        "\n",
        "The example of category prediction below uses the 20 News Groups dataset. It contains around 18000 news articles on 20 topics. The data has already been split into two subsets, one to train our model and one for testing the output of the model. For fun, we are using our own tiny set of test data instead of the provided test data.\n",
        "\n",
        "A detailed description of the dataset is available [here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html).\n",
        "\n",
        "Feel free to play around with the test input data. Although it does work a lot of the time, it's still pretty easy to trick the model. As you might expect, if you write something that isn't in one of the 5 categories it's trained on, it will spit out something that just looks random. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWQ9oY3BWp7d",
        "outputId": "8f75cf5b-9b42-4508-b8cc-fd3e366384a1"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos', \n",
        "        'rec.sport.hockey': 'Hockey', 'sci.electronics': 'Electronics', \n",
        "        'sci.med': 'Medicine'}\n",
        "\n",
        "# Get the training dataset\n",
        "# Shuffling training data is a standard practise in ML, to create more general models and prevent a common problem called overfitting. The thread linked below has a more thorough discussion\n",
        "# https://datascience.stackexchange.com/questions/24511/why-should-the-data-be-shuffled-for-machine-learning-tasks\n",
        "training_data = fetch_20newsgroups(subset='train', \n",
        "        categories=category_map.keys(), shuffle=True, random_state=5)\n",
        "\n",
        "# Get a document-term matrix\n",
        "count_vectorizer = CountVectorizer()\n",
        "train_term_counts = count_vectorizer.fit_transform(training_data.data)\n",
        "\n",
        "# We can pass a document-term matrix to tfidf.fit_transform() to get the TF-IDF weights of each word\n",
        "# Notice we didn't worry about stop words? They are going to have a very low tf-idf weight anyways.\n",
        "tfidf = TfidfTransformer()\n",
        "train_tfidf = tfidf.fit_transform(train_term_counts)\n",
        "\n",
        "# Train the model \n",
        "# For each row in our document-term matrix, we have a corresponding category in training_data.target\n",
        "classifier = MultinomialNB().fit(train_tfidf, training_data.target)\n",
        "\n",
        "# Erase my test data and create your own. Keep in mind the model is going to try to classify in one of the 5 categories in category_map\n",
        "input_data = [\n",
        "    'You should always be careful if you are driving a car', \n",
        "    'A lot of devices are not as secure as you might think',\n",
        "    'The sports cup was won by a team because they scored the most points at the super cup game, yay',\n",
        "    'Big election has politicians doing all sorts of stuff to get votes', \n",
        "    'Medical experts warn Burrata cheese sold in Quebec is not safe'\n",
        "]\n",
        "\n",
        "\n",
        "# Transform input data using count vectorizer\n",
        "input_term_counts = count_vectorizer.transform(input_data)\n",
        "\n",
        "# Transform again to get the tf-idf weights\n",
        "input_tfidf = tfidf.transform(input_term_counts)\n",
        "\n",
        "# With our data in this format, we can pass it to the classification model and see what it predicts.\n",
        "predictions = classifier.predict(input_tfidf)\n",
        "\n",
        "# Print the outputs\n",
        "for sent, category in zip(input_data, predictions):\n",
        "    print('\\nInput:', sent, '\\nPredicted category:', \n",
        "            category_map[training_data.target_names[category]])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input: You should always be careful if you are driving a car \n",
            "Predicted category: Autos\n",
            "\n",
            "Input: A lot of devices are not as secure as you might think \n",
            "Predicted category: Electronics\n",
            "\n",
            "Input: The sports cup was won by a team because they scored the most points at the super cup game, yay \n",
            "Predicted category: Hockey\n",
            "\n",
            "Input: Big election has politicians doing all sorts of stuff to get votes \n",
            "Predicted category: Politics\n",
            "\n",
            "Input: Medical experts warn Burrata cheese sold in Quebec is not safe \n",
            "Predicted category: Medicine\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v1i0UWaW50q"
      },
      "source": [
        "### Gender Identifier\n",
        "\n",
        "Gender identification is a well-studied task in NLP with many different approaches. In the example below, we will test if the model is able to accurately identify gender given the last couple letters of a first name. \n",
        "\n",
        "In classification problems (such as gender identification and category prediction), we often create the model like so: \n",
        "\n",
        "`model = whateverModelIAmUsing.fit(X, y)`\n",
        "\n",
        "or \n",
        "\n",
        "`training_data = [({featureName: feature}, target), ({featureName: feature}, target)...]`\n",
        "\n",
        "`model = whateverModelIAmUsing.train(training_data)`\n",
        "\n",
        "In the first example, `X` is the set of **features** we think will help the model make accurate predicitions and `y` is the set of **targets**, AKA the answers that the model should ideally come up with. There are many methods and heuristics out there for choosing good features (if you're interested in learning more about this, there's a good tutorial [here](https://www.kaggle.com/learn/feature-engineering)). \n",
        "\n",
        "For our purposes, let's simply compare the accuracy between a few different sets of features. We'll train the model based on the last letter of a name, the last two letters, the last three letters, and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_dJna9NW_bs",
        "outputId": "9727ee12-877e-4802-ef04-a8317bef1cbd"
      },
      "source": [
        "import random\n",
        "\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.classify import accuracy as nltk_accuracy\n",
        "from nltk.corpus import names\n",
        "\n",
        "# This time, we are only going to pass the last N letters of the word to the model. \n",
        "def extract_features(word, N=2):\n",
        "    last_n_letters = word[-N:]\n",
        "    return {'lastLetters': last_n_letters.lower()}\n",
        "\n",
        "if __name__=='__main__':\n",
        "    # Create training data using labeled names available in NLTK\n",
        "    # Unfortunately the dataset doesn't yet contain a list of gender-neutral names\n",
        "    male_list = [(name, 'male') for name in names.words('male.txt')]\n",
        "    female_list = [(name, 'female') for name in names.words('female.txt')]\n",
        "    data = (male_list + female_list)\n",
        "\n",
        "    #Shuffle the data\n",
        "    random.seed(5)\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Create test data\n",
        "    input_names = ['Yash', 'Shrimanti', 'Sai Ram', 'Riley', 'Brooke', 'Ashley', 'Robin']\n",
        "\n",
        "    # Define the number of samples used for train and test\n",
        "    # It's typical to use an 80/20 split\n",
        "    num_train = int(0.8 * len(data))\n",
        "\n",
        "    # Iterate through different lengths to compare the accuracy\n",
        "    for i in range(1, 6):\n",
        "        print('\\nNumber of end letters:', i)\n",
        "        features = [(extract_features(n, i), gender) for (n, gender) in data]\n",
        "        train_data, test_data = features[:num_train], features[num_train:]\n",
        "        classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "        # Compute the accuracy of the classifier \n",
        "        accuracy = round(100 * nltk_accuracy(classifier, test_data), 2)\n",
        "        print('Accuracy = ' + str(accuracy) + '%')\n",
        "\n",
        "        # Predict outputs for input names using the trained classifier model\n",
        "        for name in input_names:\n",
        "            print(name, '=>', classifier.classify(extract_features(name, i)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of end letters: 1\n",
            "Accuracy = 75.02%\n",
            "Yash => female\n",
            "Shrimanti => female\n",
            "Sai Ram => male\n",
            "Riley => female\n",
            "Brooke => female\n",
            "Ashley => female\n",
            "Robin => male\n",
            "\n",
            "Number of end letters: 2\n",
            "Accuracy = 78.35%\n",
            "Yash => male\n",
            "Shrimanti => female\n",
            "Sai Ram => male\n",
            "Riley => female\n",
            "Brooke => male\n",
            "Ashley => female\n",
            "Robin => male\n",
            "\n",
            "Number of end letters: 3\n",
            "Accuracy = 76.02%\n",
            "Yash => male\n",
            "Shrimanti => female\n",
            "Sai Ram => male\n",
            "Riley => male\n",
            "Brooke => female\n",
            "Ashley => male\n",
            "Robin => male\n",
            "\n",
            "Number of end letters: 4\n",
            "Accuracy = 69.35%\n",
            "Yash => female\n",
            "Shrimanti => female\n",
            "Sai Ram => female\n",
            "Riley => female\n",
            "Brooke => female\n",
            "Ashley => female\n",
            "Robin => male\n",
            "\n",
            "Number of end letters: 5\n",
            "Accuracy = 65.07%\n",
            "Yash => female\n",
            "Shrimanti => female\n",
            "Sai Ram => female\n",
            "Riley => male\n",
            "Brooke => female\n",
            "Ashley => female\n",
            "Robin => female\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znIo9MJJXQk9"
      },
      "source": [
        "### Sentiment Analyzer\n",
        "\n",
        "Sentiment analysis, or opinion mining, is the practise of creating models that determine the tone of a piece of text (or voice) data, such as whether a review was positive or negative. \n",
        "\n",
        "Below is an example of a sentiment analyzer using NLTK's Movie Review toy dataset. \n",
        "\n",
        "If you're interested/have time, sentiment analysis of tweets can be a fun project. Here's a [tutorial](https://towardsdatascience.com/how-to-scrape-tweets-from-twitter-59287e20f0f1) on how to use the twitter API to get a dataset of tweets into python. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkwfOWLcYeBZ",
        "outputId": "ab871ae9-9754-4710-811f-3efca671bb74"
      },
      "source": [
        "from nltk.corpus import movie_reviews \n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy as nltk_accuracy\n",
        " \n",
        "# Extract features from the input list of words\n",
        "# The format we are using for the features looks like this:\n",
        "# [({'here': True, 'are': True, 'all': True, 'the': True, 'words': True, 'in': True, 'the': True, 'review': True}, Positive)] \n",
        "def extract_features(words):\n",
        "    return dict([(word, True) for word in words])\n",
        " \n",
        "if __name__=='__main__':\n",
        "    # Load the reviews from the corpus \n",
        "    fileids_pos = movie_reviews.fileids('pos')\n",
        "    fileids_neg = movie_reviews.fileids('neg')\n",
        "     \n",
        "    # Extract the features from the reviews\n",
        "    features_pos = [(extract_features(movie_reviews.words(\n",
        "            fileids=[f])), 'Positive') for f in fileids_pos]\n",
        "    features_neg = [(extract_features(movie_reviews.words(\n",
        "            fileids=[f])), 'Negative') for f in fileids_neg]\n",
        "     \n",
        "    # This is our 80/20 train/test split\n",
        "    threshold = 0.8\n",
        "    num_pos = int(threshold * len(features_pos))\n",
        "    num_neg = int(threshold * len(features_neg))\n",
        "  \n",
        "    features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
        "    features_test = features_pos[num_pos:] + features_neg[num_neg:]  \n",
        "     \n",
        "    # Train a Naive Bayes classifier & get the accuracy\n",
        "    classifier = NaiveBayesClassifier.train(features_train)\n",
        "    print('\\nAccuracy of the classifier:', nltk_accuracy(\n",
        "            classifier, features_test))\n",
        "\n",
        "    # NaiveBayesClassifier can get us the most informative words, that is, words that strongly influence the model\n",
        "    top_ten_words = classifier.most_informative_features()[:10]\n",
        "    print('\\nTop ten most informative words: ')\n",
        "    for i, item in enumerate(top_ten_words):\n",
        "      print(str(i+1) + '. ' + item[0])\n",
        "\n",
        "    # Let's make up our own test data again\n",
        "    input_reviews = [\n",
        "        'I liked the cinematography', \n",
        "        'This was a terrible movie, the characters were so dumb',\n",
        "        'This movie has one of my favorite actors! I loved it!', \n",
        "        'This is such an boring movie. Would not recommend.',\n",
        "        'This movie contains Nicolas Cage'\n",
        "    ]\n",
        "\n",
        "    print(\"\\nMovie review predictions:\")\n",
        "    for review in input_reviews:\n",
        "        print(\"\\nReview:\", review)\n",
        "\n",
        "        # Compute the probabilities\n",
        "        probabilities = classifier.prob_classify(extract_features(review.split()))\n",
        "\n",
        "        # Pick the maximum value\n",
        "        predicted_sentiment = probabilities.max()\n",
        "\n",
        "        # Print outputs\n",
        "        print(\"Predicted sentiment:\", predicted_sentiment)\n",
        "        print(\"Probability:\", round(probabilities.prob(predicted_sentiment), 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy of the classifier: 0.735\n",
            "\n",
            "Top ten most informative words: \n",
            "1. outstanding\n",
            "2. insulting\n",
            "3. vulnerable\n",
            "4. ludicrous\n",
            "5. uninvolving\n",
            "6. astounding\n",
            "7. avoids\n",
            "8. fascination\n",
            "9. symbol\n",
            "10. seagal\n",
            "\n",
            "Movie review predictions:\n",
            "\n",
            "Review: I liked the cinematography\n",
            "Predicted sentiment: Positive\n",
            "Probability: 0.69\n",
            "\n",
            "Review: This was a terrible movie, the characters were so dumb\n",
            "Predicted sentiment: Negative\n",
            "Probability: 0.92\n",
            "\n",
            "Review: This movie has one of my favorite actors! I loved it!\n",
            "Predicted sentiment: Positive\n",
            "Probability: 0.56\n",
            "\n",
            "Review: This is such an boring movie. Would not recommend.\n",
            "Predicted sentiment: Negative\n",
            "Probability: 0.76\n",
            "\n",
            "Review: This movie contains Nicolas Cage\n",
            "Predicted sentiment: Negative\n",
            "Probability: 0.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0HSLjNYu1S"
      },
      "source": [
        "### Topic Modeling\n",
        "\n",
        "So far, we have seen examples of classificaton, where we have some data and we'd like to make a specific conclusion about it: positive or negative, about sports or about politics, etc. We have predetermined the categories that we want to fit our data into. \n",
        "\n",
        "Suppose we want to learn something about some given text data without having any pre-determined categories. One thing we can do is topic modeling, where we generate a statistical model that tells us what a document is about. \n",
        "\n",
        "**Latent Dirichlet Allocation** (LDA) is an algorithm for creating *topic vectors*. A topic vector is a set of words which represent an abstract topic. If you are interested in a full description of the algorithm, there is one [here.](https://www.youtube.com/watch?v=DWJYZq_fQ2A). \n",
        "\n",
        "We need pass a parameter to the LDA model function that tells it how many topics we want it to return. There is no way of determining how many noteworthy LDA topic vectors a document has; it's far from an exact science and requires some trial and error.\n",
        "\n",
        "Feel free to play around with the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiNZcCzUYzvH",
        "outputId": "1ec17870-209a-4e95-81f0-566b49359529"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer  \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from gensim import models, corpora\n",
        "\n",
        "# More of our own test data. \n",
        "def get_data():\n",
        "  return [\n",
        "          'The recorded history of Scotland begins with the arrival of the Roman Empire in the 1st century.',\n",
        "          'Then the Viking invasions began, forcing the Picts and Gaels to unite, forming the Kingdom of Scotland.',\n",
        "          'The Kingdom of Scotland was united under the House of Alpin, whose members fought among each other during frequent disputed successions.',\n",
        "          'England would take advantage of this questioned succession to launch a series of conquests, resulting in the Wars of Scottish Independence',\n",
        "          'During the Scottish Enlightenment and Industrial Revolution, Scotland became one of the powerhouses of Europe.',\n",
        "          'Giraffes usually inhabit savannahs and open woodlands.' ,\n",
        "          'The giraffe\\'s chief distinguishing characteristics are its extremely long neck and legs and its distinctive coat pattern.',\n",
        "          'Giraffes may be preyed on by lions, leopards, spotted hyenas and African wild dogs.',\n",
        "          'It is classified as vulnerable to extinction, and has been extirpated from many parts of its former range.',\n",
        "          'The elongation of the neck appears to have started early in the giraffe lineage.',\n",
        "  ];\n",
        "\n",
        "\n",
        "def preprocess(input_text):\n",
        "    # Regular expression tokenizer, we'd like to ignore punctuation and numbers\n",
        "    tokenizer = RegexpTokenizer(r'\\w+') \n",
        "    stop_words = stopwords.words('english')\n",
        "    stemmer = SnowballStemmer('english')\n",
        "\n",
        "    tokens = tokenizer.tokenize(input_text.lower()) \n",
        "    tokens = [x for x in tokens if not x in stop_words]\n",
        "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
        "\n",
        "    return tokens_stemmed\n",
        "    \n",
        "if __name__=='__main__':\n",
        "    data = get_data()\n",
        "\n",
        "    # Create a list for sentence tokens\n",
        "    tokens = [preprocess(x) for x in data]\n",
        "    \n",
        "    # Create document-term matrix\n",
        "    # In this case, we are taking the tokenized words and using a bag-of-words format to create the doc-term matrix, because there is always more than one way of doing things\n",
        "    # doc2bow => given a document, we would like a bag of words, meaning for each token create a tuple with a token ID and the number of times it occurs in the document. \n",
        "    # https://en.wikipedia.org/wiki/Bag-of-words_model\n",
        "    dict_tokens = corpora.Dictionary(tokens)\n",
        "    doc_term_matrix = [dict_tokens.doc2bow(token) for token in tokens]\n",
        "\n",
        "    # The number of topics we want the LDA model to give us, I chose 2 because it already looks like there are two topics in the dataset\n",
        "    # For most real-world applications, the dataset would be too large to guess at the 'right' number of topics. You end up just picking a number. \n",
        "    num_topics = 2\n",
        "\n",
        "    # Generate the LDA model \n",
        "    ldamodel = models.ldamodel.LdaModel(doc_term_matrix, \n",
        "            num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
        "\n",
        "    num_words = 5\n",
        "    print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
        "    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
        "        print('\\nTopic', item[0])\n",
        "\n",
        "        # Print the contributing words along with their relative contributions \n",
        "        list_of_strings = item[1].split(' + ')\n",
        "        for text in list_of_strings:\n",
        "            weight = text.split('*')[0]\n",
        "            word = text.split('*')[1]\n",
        "            print(word, '==>', str(round(float(weight) * 100, 2)) + '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 contributing words to each topic:\n",
            "\n",
            "Topic 0\n",
            "\"giraff\" ==> 4.1%\n",
            "\"neck\" ==> 2.9%\n",
            "\"scottish\" ==> 1.7%\n",
            "\"success\" ==> 1.7%\n",
            "\"seri\" ==> 1.7%\n",
            "\n",
            "Topic 1\n",
            "\"scotland\" ==> 4.9%\n",
            "\"kingdom\" ==> 2.7%\n",
            "\"unit\" ==> 2.7%\n",
            "\"success\" ==> 1.6%\n",
            "\"disput\" ==> 1.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv-TP82VeA_r"
      },
      "source": [
        "## Now go ahead and try things out on a dataset of your choice :)\n",
        "\n",
        "The tasks explained above are by no means exhaustive, there are a ton of other things you can do in NLTK. Here are some examples of other tasks/tools you might want to google: \n",
        "\n",
        "\n",
        "*   Chunking \n",
        "*   Bag of words (even thought I kind of snuck it in anyways)\n",
        "*   Word2Vec (for learning associations between words)\n",
        "*   Creating summaries\n",
        "*   Visualizing text data (if you really aren't sure what you want to do, start by making a word cloud)\n",
        "*   Similarity matching\n",
        "*   Natural language translation\n",
        "*   Lots of other stuff\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KKNtqLSan8R"
      },
      "source": [
        "# References\n",
        "\n",
        "https://kavita-ganesan.com/how-to-use-countvectorizer/#.YOiNuxNKj6M\n",
        "\n",
        "https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.YO4XURNKj6Y\n",
        "\n",
        "https://www.udemy.com/course/understand-and-practice-ai-natural-language-processing-in-python\n",
        "\n",
        "https://www.nltk.org/book/ch05.html\n",
        "\n",
        "\n"
      ]
    }
  ]
}